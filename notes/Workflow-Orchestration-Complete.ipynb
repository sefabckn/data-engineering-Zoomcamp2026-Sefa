{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b704f807",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: Kestra Workflow Orchestration\n",
    "\n",
    "### Quick Reference\n",
    "\n",
    "| Concept | Purpose | Syntax |\n",
    "|---------|---------|--------|\n",
    "| **Flow** | Complete workflow definition | `.yaml` file with `id`, `namespace` |\n",
    "| **Namespace** | Organization/grouping of flows | `namespace: company.project` |\n",
    "| **Inputs** | User-provided parameters | `inputs: - id: name, type: STRING` |\n",
    "| **Variables** | Reusable values in workflow | `{{ vars.variable_name }}` |\n",
    "| **Kestra Values (KV)** | Secure configuration storage | `{{ kv.config_name }}` |\n",
    "| **Tasks** | Execution units (ETL jobs) | `tasks: - id: task_id, type: plugin` |\n",
    "| **Plugins** | Pre-built functionality | `io.kestra.plugin.gcp.bigquery.Query` |\n",
    "| **Concurrency** | Execution limits | `concurrency: limit: 5, behavior: FAIL` |\n",
    "| **Triggers** | Schedule automation | `type: io.kestra.plugin.core.trigger.Schedule` |\n",
    "| **Cron Expression** | Scheduling syntax | `0 0 * * *` (daily at midnight) |\n",
    "\n",
    "### Workflow Development Process\n",
    "\n",
    "1. **Design** → Plan your ETL/ELT logic\n",
    "2. **Create YAML** → Define flow with namespace and ID\n",
    "3. **Add Inputs** → Accept user parameters if needed\n",
    "4. **Set Variables** → Define reusable values\n",
    "5. **Configure Tasks** → Build ETL steps with plugins\n",
    "6. **Add Triggers** → Schedule execution with cron\n",
    "7. **Set KV Values** → Store sensitive config in Kestra\n",
    "8. **Test** → Run manually first, then schedule\n",
    "9. **Monitor** → Check logs and execution history\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- ✅ Use meaningful IDs and namespaces\n",
    "- ✅ Store sensitive data in Kestra Values, not YAML\n",
    "- ✅ Set reasonable concurrency limits\n",
    "- ✅ Make tasks idempotent (safe to retry)\n",
    "- ✅ Use variables to reduce duplication\n",
    "- ✅ Add meaningful descriptions to flows\n",
    "- ✅ Start simple, then add complexity\n",
    "- ✅ Monitor first-time scheduled executions\n",
    "- ✅ Keep namespace hierarchy logical\n",
    "- ✅ Document custom task configurations\n",
    "\n",
    "---\n",
    "\n",
    "**Module 2 Complete!** You now understand how to build, schedule, and orchestrate data pipelines using Kestra with seamless integration to your GCP infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a536daa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Integrating GCP Resources with Kestra\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "Your Kestra workflows can seamlessly integrate with **Google Cloud Platform (GCP)** infrastructure that you created using Terraform in Module 1. This creates a complete data pipeline:\n",
    "\n",
    "```\n",
    "Terraform (Module 1)\n",
    "        ↓\n",
    "    Create GCP Resources\n",
    "    - Compute Engine instances\n",
    "    - Cloud Storage buckets\n",
    "    - BigQuery datasets\n",
    "    - Service accounts & permissions\n",
    "        ↓\n",
    "    Kestra (Module 2)\n",
    "        ↓\n",
    "    Orchestrate workflows using GCP resources\n",
    "```\n",
    "\n",
    "### GCP Resources Created by Terraform\n",
    "\n",
    "From Module 1, you should have created:\n",
    "\n",
    "1. **Cloud Storage Bucket**: For storing raw and processed data\n",
    "2. **BigQuery Dataset**: For data warehousing and analytics\n",
    "3. **Compute Engine Instance**: For running compute-intensive jobs\n",
    "4. **Service Account**: For authentication and authorization\n",
    "\n",
    "### Kestra Values for GCP Configuration\n",
    "\n",
    "Store GCP configuration as Kestra Values (KV) instead of hardcoding them:\n",
    "\n",
    "```yaml\n",
    "# GCP Configuration to be stored in Kestra Values:\n",
    "kv.gcp_project_id      # Your GCP project ID\n",
    "kv.gcp_location        # Region (e.g., \"us-central1\", \"europe-west1\")\n",
    "kv.gcp_bucket_name     # Cloud Storage bucket name\n",
    "kv.gcp_dataset         # BigQuery dataset name\n",
    "```\n",
    "\n",
    "### Step 1: Create Kestra Values in Kestra UI\n",
    "\n",
    "1. Go to **Admin → Kestra Values** (or similar menu option)\n",
    "2. Create new values:\n",
    "   - **Name**: `gcp_project_id` | **Value**: `your-project-id`\n",
    "   - **Name**: `gcp_location` | **Value**: `us-central1`\n",
    "   - **Name**: `gcp_bucket_name` | **Value**: `my-data-bucket`\n",
    "   - **Name**: `gcp_dataset` | **Value**: `analytics_warehouse`\n",
    "\n",
    "### Step 2: Reference in Workflows\n",
    "\n",
    "```yaml\n",
    "id: gcp_integrated_pipeline\n",
    "namespace: data_engineering.gcp\n",
    "\n",
    "description: \"ETL pipeline using Terraform-created GCP resources\"\n",
    "\n",
    "tasks:\n",
    "  - id: upload_to_storage\n",
    "    type: io.kestra.plugin.gcp.storage.Upload\n",
    "    projectId: \"{{ kv.gcp_project_id }}\"\n",
    "    bucket: \"{{ kv.gcp_bucket_name }}\"\n",
    "    from: \"/local/data/taxi_trips.csv\"\n",
    "    to: \"raw/taxi_trips/{{ vars.execution_date }}/data.csv\"\n",
    "  \n",
    "  - id: load_to_bigquery\n",
    "    type: io.kestra.plugin.gcp.bigquery.LoadFromGCS\n",
    "    projectId: \"{{ kv.gcp_project_id }}\"\n",
    "    dataset: \"{{ kv.gcp_dataset }}\"\n",
    "    table: \"taxi_trips_raw\"\n",
    "    from: \"gs://{{ kv.gcp_bucket_name }}/raw/taxi_trips/*/data.csv\"\n",
    "    format: \"CSV\"\n",
    "    csvOptions:\n",
    "      skipLeadingRows: 1\n",
    "  \n",
    "  - id: transform_in_bigquery\n",
    "    type: io.kestra.plugin.gcp.bigquery.Query\n",
    "    projectId: \"{{ kv.gcp_project_id }}\"\n",
    "    dataset: \"{{ kv.gcp_dataset }}\"\n",
    "    sql: |\n",
    "      CREATE OR REPLACE TABLE `{{ kv.gcp_project_id }}.{{ kv.gcp_dataset }}.taxi_trips_clean` AS\n",
    "      SELECT\n",
    "        CAST(pickup_time AS TIMESTAMP) as pickup_time,\n",
    "        CAST(fare_amount AS FLOAT64) as fare_amount,\n",
    "        CAST(distance AS FLOAT64) as distance,\n",
    "        passenger_count,\n",
    "        CURRENT_TIMESTAMP() as processed_at\n",
    "      FROM `{{ kv.gcp_project_id }}.{{ kv.gcp_dataset }}.taxi_trips_raw`\n",
    "      WHERE fare_amount > 0 AND distance > 0\n",
    "```\n",
    "\n",
    "### Complete GCP Integration Example\n",
    "\n",
    "```yaml\n",
    "id: complete_gcp_etl\n",
    "namespace: data_engineering.gcp\n",
    "\n",
    "description: \"Complete ETL using Terraform-provisioned GCP resources\"\n",
    "\n",
    "variables:\n",
    "  execution_date: \"{{ now | dateFormat('yyyy-MM-dd') }}\"\n",
    "  raw_folder: \"raw/taxi\"\n",
    "  processed_folder: \"processed/taxi\"\n",
    "\n",
    "triggers:\n",
    "  - id: daily_run\n",
    "    type: io.kestra.plugin.core.trigger.Schedule\n",
    "    cron: \"0 3 * * *\"  # 3 AM daily\n",
    "\n",
    "tasks:\n",
    "  # Step 1: Extract from source\n",
    "  - id: extract_data\n",
    "    type: io.kestra.plugin.scripts.python.Script\n",
    "    script: |\n",
    "      import requests\n",
    "      import csv\n",
    "      \n",
    "      # Download data from API\n",
    "      response = requests.get(\"https://api.example.com/taxi?date={{ vars.execution_date }}\")\n",
    "      \n",
    "      # Save to local file\n",
    "      with open('/tmp/taxi_data.csv', 'w') as f:\n",
    "          writer = csv.writer(f)\n",
    "          writer.writerows(response.json())\n",
    "      \n",
    "      print(f\"Extracted data for {{{ vars.execution_date }}}\")\n",
    "  \n",
    "  # Step 2: Upload to Cloud Storage\n",
    "  - id: upload_raw_data\n",
    "    type: io.kestra.plugin.gcp.storage.Upload\n",
    "    projectId: \"{{ kv.gcp_project_id }}\"\n",
    "    bucket: \"{{ kv.gcp_bucket_name }}\"\n",
    "    from: \"/tmp/taxi_data.csv\"\n",
    "    to: \"{{ vars.raw_folder }}/{{ vars.execution_date }}/data.csv\"\n",
    "    dependsOn:\n",
    "      - extract_data\n",
    "  \n",
    "  # Step 3: Load into BigQuery\n",
    "  - id: load_to_warehouse\n",
    "    type: io.kestra.plugin.gcp.bigquery.LoadFromGCS\n",
    "    projectId: \"{{ kv.gcp_project_id }}\"\n",
    "    dataset: \"{{ kv.gcp_dataset }}\"\n",
    "    table: \"taxi_trips_staging\"\n",
    "    from: \"gs://{{ kv.gcp_bucket_name }}/{{ vars.raw_folder }}/{{ vars.execution_date }}/data.csv\"\n",
    "    format: \"CSV\"\n",
    "    csvOptions:\n",
    "      skipLeadingRows: 1\n",
    "    dependsOn:\n",
    "      - upload_raw_data\n",
    "  \n",
    "  # Step 4: Transform and clean\n",
    "  - id: transform_data\n",
    "    type: io.kestra.plugin.gcp.bigquery.Query\n",
    "    projectId: \"{{ kv.gcp_project_id }}\"\n",
    "    dataset: \"{{ kv.gcp_dataset }}\"\n",
    "    sql: |\n",
    "      INSERT INTO `{{ kv.gcp_project_id }}.{{ kv.gcp_dataset }}.taxi_trips_clean`\n",
    "      SELECT\n",
    "        pickup_time,\n",
    "        dropoff_time,\n",
    "        SAFE.FLOAT64(fare_amount) as fare_amount,\n",
    "        SAFE.FLOAT64(total_amount) as total_amount,\n",
    "        passenger_count,\n",
    "        trip_distance,\n",
    "        CURRENT_TIMESTAMP() as processed_at,\n",
    "        '{{ vars.execution_date }}' as batch_date\n",
    "      FROM `{{ kv.gcp_project_id }}.{{ kv.gcp_dataset }}.taxi_trips_staging`\n",
    "      WHERE fare_amount > 0\n",
    "        AND trip_distance > 0\n",
    "        AND passenger_count > 0\n",
    "    dependsOn:\n",
    "      - load_to_warehouse\n",
    "  \n",
    "  # Step 5: Archive processed data\n",
    "  - id: archive_processed\n",
    "    type: io.kestra.plugin.gcp.storage.Copy\n",
    "    projectId: \"{{ kv.gcp_project_id }}\"\n",
    "    from: \"gs://{{ kv.gcp_bucket_name }}/{{ vars.raw_folder }}/{{ vars.execution_date }}/data.csv\"\n",
    "    to: \"gs://{{ kv.gcp_bucket_name }}/{{ vars.processed_folder }}/{{ vars.execution_date }}/data.csv\"\n",
    "    dependsOn:\n",
    "      - transform_data\n",
    "```\n",
    "\n",
    "### Key Advantages of This Integration\n",
    "\n",
    "1. **Infrastructure as Code**: Terraform creates repeatable, version-controlled infrastructure\n",
    "2. **Secure Configuration**: Kestra Values keep sensitive data out of YAML files\n",
    "3. **Scalability**: GCP handles storage and compute at scale\n",
    "4. **Auditability**: Both Terraform and Kestra maintain execution logs\n",
    "5. **Flexibility**: Easy to switch between environments (dev/prod)\n",
    "\n",
    "### Environment-Specific Workflows\n",
    "\n",
    "You can manage multiple environments by creating different Kestra Values sets:\n",
    "\n",
    "```\n",
    "Development Environment:\n",
    "  kv.gcp_project_id = \"my-project-dev\"\n",
    "  kv.gcp_bucket_name = \"my-data-dev\"\n",
    "\n",
    "Production Environment:\n",
    "  kv.gcp_project_id = \"my-project-prod\"\n",
    "  kv.gcp_bucket_name = \"my-data-prod\"\n",
    "\n",
    "# Same workflow YAML, different Kestra Values = different behavior\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150c02e9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Setting Up Triggers with Cron Expressions\n",
    "\n",
    "### What are Triggers?\n",
    "\n",
    "**Triggers** define when a workflow should automatically execute. In Kestra, you can schedule workflows using cron expressions through the **Schedule** plugin.\n",
    "\n",
    "### The Schedule Plugin\n",
    "\n",
    "The plugin to use for scheduling is: `io.kestra.plugin.core.trigger.Schedule`\n",
    "\n",
    "This is Kestra's built-in scheduling mechanism that uses standard cron syntax.\n",
    "\n",
    "### Cron Expression Syntax\n",
    "\n",
    "A cron expression has 5 fields (note: Kestra uses 5-field format, not the traditional 6-field):\n",
    "\n",
    "```\n",
    "* * * * *\n",
    "│ │ │ │ │\n",
    "│ │ │ │ └─ Day of Week (0=Sunday, 6=Saturday)\n",
    "│ │ │ └─── Month (1-12)\n",
    "│ │ └───── Day of Month (1-31)\n",
    "│ └─────── Hour (0-23)\n",
    "└───────── Minute (0-59)\n",
    "```\n",
    "\n",
    "### Special Character: Wildcard (*)\n",
    "\n",
    "The **asterisk (*)** means **\"any value\"** for that field. If you want a field to match any value, use `*`.\n",
    "\n",
    "### Cron Expression Examples\n",
    "\n",
    "| Expression | Description |\n",
    "|------------|-------------|\n",
    "| `0 0 * * *` | Daily at midnight (00:00) |\n",
    "| `0 9 * * 1` | Every Monday at 9:00 AM |\n",
    "| `*/15 * * * *` | Every 15 minutes |\n",
    "| `0 2 1 * *` | First day of every month at 2:00 AM |\n",
    "| `0 12 * * 1-5` | Weekdays at noon |\n",
    "| `30 * * * *` | Every hour at the 30-minute mark |\n",
    "| `0 0,12 * * *` | Twice daily at midnight and noon |\n",
    "\n",
    "### Trigger Configuration in YAML\n",
    "\n",
    "```yaml\n",
    "id: daily_data_refresh\n",
    "namespace: data_engineering.scheduled\n",
    "\n",
    "description: \"Daily data refresh pipeline\"\n",
    "\n",
    "triggers:\n",
    "  - id: schedule_daily\n",
    "    type: io.kestra.plugin.core.trigger.Schedule\n",
    "    cron: \"0 0 * * *\"  # Run at midnight every day\n",
    "```\n",
    "\n",
    "### Complete Example: Scheduled Taxi Data Pipeline\n",
    "\n",
    "```yaml\n",
    "id: taxi_data_daily_pipeline\n",
    "namespace: data_engineering.etl.taxi\n",
    "description: \"Extract and transform taxi data daily\"\n",
    "\n",
    "variables:\n",
    "  table_name: \"taxi_trips\"\n",
    "  partition_field: \"trip_date\"\n",
    "\n",
    "triggers:\n",
    "  - id: daily_schedule\n",
    "    type: io.kestra.plugin.core.trigger.Schedule\n",
    "    cron: \"0 2 * * *\"  # Run at 2:00 AM every day\n",
    "\n",
    "tasks:\n",
    "  - id: extract_yesterday\n",
    "    type: io.kestra.plugin.gcp.bigquery.Query\n",
    "    projectId: \"{{ kv.gcp_project_id }}\"\n",
    "    dataset: \"{{ kv.gcp_dataset }}\"\n",
    "    sql: |\n",
    "      SELECT \n",
    "        * \n",
    "      FROM `raw.taxi_data`\n",
    "      WHERE DATE(pickup_time) = CURRENT_DATE() - 1\n",
    "    destination: \"{{ vars.table_name }}_raw\"\n",
    "  \n",
    "  - id: transform_data\n",
    "    type: io.kestra.plugin.scripts.python.Script\n",
    "    dependsOn:\n",
    "      - extract_yesterday\n",
    "    script: |\n",
    "      from google.cloud import bigquery\n",
    "      client = bigquery.Client(project=\"{{ kv.gcp_project_id }}\")\n",
    "      \n",
    "      query = \"\"\"\n",
    "      INSERT INTO `{{ kv.gcp_dataset }}.{{ vars.table_name }}_clean`\n",
    "      SELECT\n",
    "        *,\n",
    "        CAST(fare_amount AS FLOAT64) as fare,\n",
    "        CURRENT_TIMESTAMP() as processed_at\n",
    "      FROM `{{ kv.gcp_dataset }}.{{ vars.table_name }}_raw`\n",
    "      WHERE fare_amount > 0\n",
    "      \"\"\"\n",
    "      \n",
    "      client.query(query).result()\n",
    "      print(\"Data transformation completed\")\n",
    "```\n",
    "\n",
    "### Cron Expression Tips\n",
    "\n",
    "1. **Use UTC Time**: Cron times are typically in UTC. Account for timezone differences\n",
    "2. **Test Before Deploying**: Use online cron validators to verify expressions\n",
    "3. **Start Simple**: Begin with basic schedules before moving to complex ones\n",
    "4. **Monitor First Runs**: Check logs to ensure the schedule triggers as expected\n",
    "5. **Multiple Triggers**: You can add multiple triggers to one flow for different schedules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e831e98c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Understanding Task Types and Plugins\n",
    "\n",
    "### What are Kestra Plugins?\n",
    "\n",
    "**Plugins** are pre-built modules in Kestra that provide functionality for specific tools, services, or operations. The `type` of a task specifies which plugin to use.\n",
    "\n",
    "### Plugin Concept\n",
    "\n",
    "Think of plugins like Python library imports. Just as you'd write:\n",
    "```python\n",
    "from pandas import read_csv\n",
    "from gcp.bigquery import Client\n",
    "```\n",
    "\n",
    "In Kestra, you specify the plugin in the `type` field:\n",
    "```yaml\n",
    "type: io.kestra.plugin.scripts.python.Script\n",
    "type: io.kestra.plugin.gcp.bigquery.Query\n",
    "```\n",
    "\n",
    "### How Task Types Work\n",
    "\n",
    "The `type` field uses a hierarchical naming convention: `io.kestra.plugin.[category].[service].[operation]`\n",
    "\n",
    "| Type | Purpose | Example |\n",
    "|------|---------|---------|\n",
    "| **scripts.python.Script** | Execute Python code | Data processing, transformations |\n",
    "| **scripts.bash.Commands** | Execute bash/shell commands | File operations, CLI tools |\n",
    "| **databases.sql.Query** | Execute SQL queries | PostgreSQL, MySQL operations |\n",
    "| **gcp.bigquery.Query** | BigQuery operations | Analytics, data warehousing |\n",
    "| **gcp.storage.Upload** | GCP Storage operations | Upload files to Cloud Storage |\n",
    "| **gcp.gcs.Upload** | Google Cloud Storage | Alternative to Upload |\n",
    "| **http.Request** | Make HTTP requests | API calls, webhooks |\n",
    "| **slack.Send** | Send Slack messages | Notifications |\n",
    "\n",
    "### Python Script Plugin Example\n",
    "\n",
    "```yaml\n",
    "tasks:\n",
    "  - id: process_with_python\n",
    "    type: io.kestra.plugin.scripts.python.Script\n",
    "    script: |\n",
    "      import pandas as pd\n",
    "      import numpy as np\n",
    "      \n",
    "      # Your Python code here\n",
    "      df = pd.read_csv('data.csv')\n",
    "      result = df[df['value'] > 100]\n",
    "      print(f\"Filtered {len(result)} rows\")\n",
    "```\n",
    "\n",
    "### BigQuery Plugin Example\n",
    "\n",
    "```yaml\n",
    "tasks:\n",
    "  - id: query_bigquery\n",
    "    type: io.kestra.plugin.gcp.bigquery.Query\n",
    "    projectId: \"{{ kv.gcp_project_id }}\"\n",
    "    dataset: \"{{ kv.gcp_dataset }}\"\n",
    "    sql: |\n",
    "      SELECT \n",
    "        date,\n",
    "        COUNT(*) as trip_count,\n",
    "        AVG(fare_amount) as avg_fare\n",
    "      FROM `{{ kv.gcp_project_id }}.{{ kv.gcp_dataset }}.trips`\n",
    "      WHERE date >= '2024-01-01'\n",
    "      GROUP BY date\n",
    "```\n",
    "\n",
    "### Cloud Storage Plugin Example\n",
    "\n",
    "```yaml\n",
    "tasks:\n",
    "  - id: upload_to_gcp\n",
    "    type: io.kestra.plugin.gcp.storage.Upload\n",
    "    projectId: \"{{ kv.gcp_project_id }}\"\n",
    "    bucket: \"{{ kv.gcp_bucket_name }}\"\n",
    "    from: \"/local/path/file.parquet\"\n",
    "    to: \"processed/file.parquet\"\n",
    "```\n",
    "\n",
    "### Accessing Plugin Documentation\n",
    "\n",
    "Each plugin has its own configuration parameters. In Kestra UI, you can:\n",
    "- Browse available plugins\n",
    "- View required vs. optional parameters\n",
    "- See example configurations for each plugin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affe3f63",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Creating and Configuring Tasks\n",
    "\n",
    "### What are Tasks?\n",
    "\n",
    "**Tasks** are the core execution units in Kestra where the real work happens. This is where you define your **ETL/ELT jobs** (Extract, Transform, Load), database operations, API calls, and any other computational work.\n",
    "\n",
    "### Task Structure Requirements\n",
    "\n",
    "Every task must have:\n",
    "1. **id**: A unique identifier within the flow\n",
    "2. **type**: The plugin type that defines what the task does\n",
    "\n",
    "```yaml\n",
    "tasks:\n",
    "  - id: unique_task_id\n",
    "    type: io.kestra.plugin.type.SubType\n",
    "    # Additional configuration specific to the type\n",
    "```\n",
    "\n",
    "### Basic Task Example\n",
    "\n",
    "```yaml\n",
    "id: etl_pipeline\n",
    "namespace: data_engineering.etl\n",
    "\n",
    "tasks:\n",
    "  - id: extract_data\n",
    "    type: io.kestra.plugin.scripts.python.Script\n",
    "    script: |\n",
    "      import pandas as pd\n",
    "      data = pd.read_csv('source.csv')\n",
    "      print(f\"Extracted {len(data)} rows\")\n",
    "  \n",
    "  - id: transform_data\n",
    "    type: io.kestra.plugin.scripts.python.Script\n",
    "    script: |\n",
    "      import pandas as pd\n",
    "      data = pd.read_csv('source.csv')\n",
    "      # Apply transformations\n",
    "      data['date'] = pd.to_datetime(data['date'])\n",
    "      data.to_csv('transformed.csv', index=False)\n",
    "  \n",
    "  - id: load_data\n",
    "    type: io.kestra.plugin.databases.sql.Query\n",
    "    url: \"jdbc:postgresql://localhost:5432/mydb\"\n",
    "    username: \"user\"\n",
    "    password: \"pass\"\n",
    "    sql: \"COPY my_table FROM 'transformed.csv';\"\n",
    "```\n",
    "\n",
    "### Task Dependencies\n",
    "\n",
    "Tasks can be sequenced so one task waits for another to complete:\n",
    "\n",
    "```yaml\n",
    "tasks:\n",
    "  - id: step1_extract\n",
    "    type: io.kestra.plugin.scripts.python.Script\n",
    "    script: |\n",
    "      print(\"Extracting data...\")\n",
    "  \n",
    "  - id: step2_transform\n",
    "    type: io.kestra.plugin.scripts.python.Script\n",
    "    dependsOn:  # This task waits for step1_extract to complete\n",
    "      - step1_extract\n",
    "    script: |\n",
    "      print(\"Transforming data...\")\n",
    "  \n",
    "  - id: step3_load\n",
    "    type: io.kestra.plugin.scripts.python.Script\n",
    "    dependsOn:\n",
    "      - step2_transform\n",
    "    script: |\n",
    "      print(\"Loading data...\")\n",
    "```\n",
    "\n",
    "### Task Best Practices\n",
    "\n",
    "- **Meaningful IDs**: Use names that describe what the task does (e.g., `extract_taxi_data`, not `task1`)\n",
    "- **Single Responsibility**: Each task should do one thing well\n",
    "- **Error Handling**: Configure retry policies and failure behaviors\n",
    "- **Idempotency**: Make tasks safe to run multiple times (important if retries occur)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fcf164",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Configuring Concurrency and Execution Behavior\n",
    "\n",
    "### What is Concurrency?\n",
    "\n",
    "**Concurrency** controls how many instances of your workflow can execute simultaneously. It sets limits on the total number of concurrent executions throughout the workflow's lifecycle.\n",
    "\n",
    "### Concurrency Settings\n",
    "\n",
    "```yaml\n",
    "id: concurrent_workflow\n",
    "namespace: examples\n",
    "\n",
    "concurrency:\n",
    "  limit: 5  # Maximum 5 concurrent executions\n",
    "  behavior: FAIL  # What to do when limit is reached\n",
    "```\n",
    "\n",
    "### Behavior Options\n",
    "\n",
    "When the concurrency limit is reached, Kestra can handle new execution requests in different ways:\n",
    "\n",
    "| Behavior | Description |\n",
    "|----------|-------------|\n",
    "| **FAIL** | New executions fail immediately with an error |\n",
    "| **QUEUE** | New executions wait in queue until a slot becomes available |\n",
    "| **FAIL_AFTER_N_RETRIES** | Retry N times before failing |\n",
    "\n",
    "### Real-World Example\n",
    "\n",
    "```yaml\n",
    "id: data_processing_pipeline\n",
    "namespace: data_engineering.etl\n",
    "\n",
    "concurrency:\n",
    "  limit: 3\n",
    "  behavior: FAIL\n",
    "\n",
    "description: |\n",
    "  This workflow processes data files.\n",
    "  - Maximum 3 files can be processed in parallel\n",
    "  - If a 4th execution is triggered, it will fail immediately\n",
    "  - Once one completes, a queued execution can start\n",
    "```\n",
    "\n",
    "### Use Cases for Concurrency Limits\n",
    "\n",
    "- **Resource Management**: Prevent overwhelming your database or storage\n",
    "- **Rate Limiting**: Respect API rate limits of external services\n",
    "- **Cost Control**: Limit concurrent cloud resource usage (especially on GCP)\n",
    "- **Sequential Processing**: Set limit to 1 for workflows that must run one at a time\n",
    "\n",
    "### Example: Cost-Conscious GCP Workflow\n",
    "\n",
    "```yaml\n",
    "id: gcp_bigquery_job\n",
    "namespace: data_engineering.gcp\n",
    "\n",
    "concurrency:\n",
    "  limit: 2  # Only 2 BigQuery jobs at a time to control costs\n",
    "  behavior: QUEUE\n",
    "\n",
    "tasks:\n",
    "  - id: run_bigquery_job\n",
    "    type: io.kestra.plugin.gcp.bigquery.Query\n",
    "    projectId: \"{{ kv.gcp_project_id }}\"\n",
    "    sql: \"SELECT COUNT(*) FROM `{{ kv.gcp_dataset }}.large_table`\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3193da",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Working with Variables and Kestra Values\n",
    "\n",
    "### What are Variables?\n",
    "\n",
    "**Variables** are dynamic values that you can define and reference throughout your workflow. Once created, they can be accessed anywhere in the flow, making your workflows cleaner and more maintainable.\n",
    "\n",
    "### Variable Syntax\n",
    "\n",
    "To reference a variable in your flow, use the following syntax:\n",
    "```\n",
    "{{ vars.variable_name }}\n",
    "```\n",
    "\n",
    "### Defining and Using Variables\n",
    "\n",
    "```yaml\n",
    "id: workflow_with_variables\n",
    "namespace: examples\n",
    "\n",
    "variables:\n",
    "  project_name: \"taxi_project\"\n",
    "  batch_size: 1000\n",
    "  output_format: \"parquet\"\n",
    "\n",
    "tasks:\n",
    "  - id: log_variables\n",
    "    type: io.kestra.plugin.scripts.python.Script\n",
    "    script: |\n",
    "      print(\"Project: {{ vars.project_name }}\")\n",
    "      print(\"Batch Size: {{ vars.batch_size }}\")\n",
    "      print(\"Output: {{ vars.output_format }}\")\n",
    "```\n",
    "\n",
    "### Benefits of Variables\n",
    "\n",
    "- **Reusability**: Define once, use everywhere\n",
    "- **Maintainability**: Change in one place affects all references\n",
    "- **Reduced Duplication**: No need to repeat the same value multiple times\n",
    "- **Clarity**: Makes flows easier to understand\n",
    "\n",
    "---\n",
    "\n",
    "### What are Kestra Values (KV)?\n",
    "\n",
    "**Kestra Values (KV)** are a special type of variable used to store **sensitive** or **configuration** information that needs to persist across multiple workflow executions. They are stored securely in Kestra's database.\n",
    "\n",
    "### Creating Kestra Values\n",
    "\n",
    "Kestra Values are typically created through the Kestra UI or API, not directly in YAML files.\n",
    "\n",
    "### Common Use Cases for KV\n",
    "\n",
    "In the context of GCP integration:\n",
    "- `gcp_project_id`: Your GCP project ID\n",
    "- `gcp_location`: GCP compute region (e.g., `us-central1`)\n",
    "- `gcp_bucket_name`: Cloud Storage bucket name\n",
    "- `gcp_dataset`: BigQuery dataset name\n",
    "- `gcp_credentials`: Service account credentials (sensitive!)\n",
    "\n",
    "### Referencing Kestra Values in Flows\n",
    "\n",
    "```yaml\n",
    "id: gcp_workflow\n",
    "namespace: data_engineering.gcp\n",
    "\n",
    "tasks:\n",
    "  - id: create_bucket\n",
    "    type: io.kestra.plugin.gcp.storage.CreateBucket\n",
    "    projectId: \"{{ kv.gcp_project_id }}\"\n",
    "    name: \"{{ kv.gcp_bucket_name }}\"\n",
    "    location: \"{{ kv.gcp_location }}\"\n",
    "  \n",
    "  - id: query_dataset\n",
    "    type: io.kestra.plugin.gcp.bigquery.Query\n",
    "    projectId: \"{{ kv.gcp_project_id }}\"\n",
    "    dataset: \"{{ kv.gcp_dataset }}\"\n",
    "    sql: \"SELECT * FROM my_table\"\n",
    "```\n",
    "\n",
    "### Why Use KV for GCP Configuration?\n",
    "\n",
    "- **Security**: Credentials and sensitive data are not hardcoded in YAML\n",
    "- **Centralized Management**: Change GCP project IDs in one place\n",
    "- **Environment Flexibility**: Use different KV values for dev/prod environments\n",
    "- **Audit Trail**: Track changes to critical configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8f1bf6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Understanding Namespaces and Unique Identifiers\n",
    "\n",
    "### What are Namespaces?\n",
    "\n",
    "A **namespace** is a logical grouping or labeling system for organizing workflows. It helps distinguish between different workflows and serves as a hierarchical organization mechanism.\n",
    "\n",
    "### Purpose of Namespaces\n",
    "\n",
    "- **Organization**: Group related workflows together\n",
    "- **Distinction**: Prevent naming conflicts across different projects\n",
    "- **Navigation**: Make it easier to find and manage workflows in the UI\n",
    "- **Access Control**: Can be used for permission management\n",
    "\n",
    "### Namespace Convention\n",
    "\n",
    "Use dot notation to create hierarchies:\n",
    "```\n",
    "namespace: company.department.project\n",
    "namespace: data_engineering.etl.taxi\n",
    "namespace: analytics.reporting\n",
    "```\n",
    "\n",
    "### Unique ID Requirement\n",
    "\n",
    "Every flow **must** have a unique `id` within its namespace. The combination of `namespace + id` creates a globally unique identifier.\n",
    "\n",
    "### Example: Namespace and ID Usage\n",
    "\n",
    "```yaml\n",
    "# Flow 1: Extract raw data\n",
    "id: extract_taxi_data\n",
    "namespace: data_engineering.etl.taxi\n",
    "description: \"Extract raw taxi data from source\"\n",
    "\n",
    "# Flow 2: Transform data\n",
    "id: transform_taxi_data\n",
    "namespace: data_engineering.etl.taxi\n",
    "description: \"Transform and clean taxi data\"\n",
    "\n",
    "# Flow 3: Load to warehouse\n",
    "id: load_taxi_data\n",
    "namespace: data_engineering.etl.taxi\n",
    "description: \"Load transformed data to warehouse\"\n",
    "\n",
    "# Flow 4: Another project with same task name (different namespace)\n",
    "id: extract_data  # Same ID as Flow 1 would be, but different namespace\n",
    "namespace: analytics.raw_data\n",
    "description: \"Extract raw analytics data\"\n",
    "```\n",
    "\n",
    "### Key Rules\n",
    "\n",
    "1. **No Duplicate IDs**: You cannot save two flows with the same ID in the same namespace\n",
    "2. **Namespace is Required**: Every flow must have a namespace\n",
    "3. **Hierarchy Pattern**: Use dot notation to create logical groupings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1c8c24",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Defining Inputs and Managing Data Types\n",
    "\n",
    "### What are Inputs?\n",
    "\n",
    "**Inputs** allow users to provide values to your workflow when triggering it manually or programmatically. They act as parameters that customize the workflow execution.\n",
    "\n",
    "### Supported Data Types\n",
    "\n",
    "Kestra supports multiple input types to store different kinds of data:\n",
    "\n",
    "| Type | Description | Example |\n",
    "|------|-------------|---------|\n",
    "| **String** | Text values | `\"2024-02-05\"`, `\"my-bucket\"` |\n",
    "| **Integer** | Whole numbers | `100`, `365` |\n",
    "| **Float** | Decimal numbers | `3.14`, `99.99` |\n",
    "| **Date** | Calendar dates | `2024-02-05` |\n",
    "| **Boolean** | True/False values | `true`, `false` |\n",
    "| **Array** | List of values | `[1, 2, 3]` |\n",
    "| **Object** | Structured data | `{\"key\": \"value\"}` |\n",
    "\n",
    "### Example Input Definition\n",
    "\n",
    "```yaml\n",
    "id: user_input_flow\n",
    "namespace: examples\n",
    "\n",
    "inputs:\n",
    "  - id: execution_date\n",
    "    type: DATE\n",
    "    description: \"Date to process data for\"\n",
    "    defaults: \"2024-02-05\"\n",
    "  \n",
    "  - id: dataset_name\n",
    "    type: STRING\n",
    "    description: \"Name of the dataset\"\n",
    "    required: true\n",
    "  \n",
    "  - id: batch_size\n",
    "    type: INT\n",
    "    description: \"Number of records to process\"\n",
    "    defaults: 1000\n",
    "```\n",
    "\n",
    "### Accessing Inputs in Tasks\n",
    "\n",
    "Inputs are accessed within tasks using the `inputs` property:\n",
    "```yaml\n",
    "tasks:\n",
    "  - id: process_data\n",
    "    type: io.kestra.plugin.scripts.python.Script\n",
    "    script: |\n",
    "      print(\"Processing date: {{ inputs.execution_date }}\")\n",
    "      print(\"Dataset: {{ inputs.dataset_name }}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493f73fa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Kestra Flow Structure and YAML Configuration\n",
    "\n",
    "### Understanding Flows\n",
    "\n",
    "A **flow** is the core unit in Kestra that defines your entire workflow. Each flow is stored as a **YAML file** with a `.yaml` extension.\n",
    "\n",
    "### Basic Flow Structure\n",
    "\n",
    "```yaml\n",
    "id: unique_flow_identifier\n",
    "namespace: my.project.namespace\n",
    "description: \"Brief description of what this flow does\"\n",
    "\n",
    "inputs: {}\n",
    "variables: {}\n",
    "tasks: []\n",
    "triggers: []\n",
    "```\n",
    "\n",
    "### Key Points\n",
    "\n",
    "- **File Extension**: Each flow must be saved as a `.yaml` file\n",
    "- **Unique ID**: Every flow must have a unique `id`. You **cannot** have two different YAML files with the same ID\n",
    "- **Namespace**: Serves as a logical grouping mechanism (explained in Section 4)\n",
    "- **Structure**: The flow file contains all configuration in one place\n",
    "\n",
    "### File Naming Convention\n",
    "\n",
    "While the `id` property is what matters internally, it's best practice to name your YAML file descriptively:\n",
    "- `01_Hello_World.yaml` - Simple starter flow\n",
    "- `04_postgres_taxi.yaml` - PostgreSQL database operations\n",
    "- `08_gcp_taxi.yaml` - GCP integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf09f16",
   "metadata": {},
   "source": [
    "## 1. Overview of Kestra and Workflow Orchestration\n",
    "\n",
    "### What is Kestra?\n",
    "\n",
    "**Kestra** is an **open-source workflow orchestration tool** designed to automate, schedule, and manage data pipelines and ETL/ELT jobs. It provides a centralized platform for defining, executing, and monitoring complex workflows.\n",
    "\n",
    "### Purpose of Workflow Orchestration\n",
    "\n",
    "Workflow orchestration enables you to:\n",
    "- **Automate** repetitive data processing tasks\n",
    "- **Schedule** jobs to run at specific times using cron expressions\n",
    "- **Manage dependencies** between tasks\n",
    "- **Monitor** execution history and logs\n",
    "- **Handle failures** gracefully with retry mechanisms\n",
    "- **Scale** multiple concurrent executions\n",
    "\n",
    "### Why Kestra?\n",
    "\n",
    "- **Open Source**: Free to use and modify\n",
    "- **Cloud-Native**: Easy integration with cloud services (GCP, AWS, Azure)\n",
    "- **Simple YAML-Based Configuration**: No complex coding required\n",
    "- **Rich Plugin Ecosystem**: Support for various tools and databases\n",
    "- **Flexible Scheduling**: Advanced cron expression support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c2fc93",
   "metadata": {},
   "source": [
    "# Workflow Orchestration with Kestra\n",
    "\n",
    "## Module 2: Complete Guide\n",
    "\n",
    "**Overview**: This notebook documents comprehensive notes on Kestra workflow orchestration, covering flow structure, configuration, task management, and integration with cloud infrastructure."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
